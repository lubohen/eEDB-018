{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Nome da Faixa</th>\n",
       "      <td>Bulldog Down in Sunny Tennessee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cantor ou Compositor</th>\n",
       "      <td>Charlie Poole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Genero</th>\n",
       "      <td>Country; International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quadrante</th>\n",
       "      <td>Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acousticness</th>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>danceability</th>\n",
       "      <td>0.649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>energy</th>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instrumentalness</th>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liveness</th>\n",
       "      <td>0.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loudness</th>\n",
       "      <td>-12.237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mode</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speechiness</th>\n",
       "      <td>0.0417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tempo</th>\n",
       "      <td>167.348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_signature</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valence</th>\n",
       "      <td>0.903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0\n",
       "Nome da Faixa         Bulldog Down in Sunny Tennessee\n",
       "Cantor ou Compositor                    Charlie Poole\n",
       "Genero                         Country; International\n",
       "Quadrante                                          Q3\n",
       "acousticness                                     0.96\n",
       "danceability                                    0.649\n",
       "energy                                           0.39\n",
       "instrumentalness                             0.000003\n",
       "key                                                 5\n",
       "liveness                                        0.096\n",
       "loudness                                      -12.237\n",
       "mode                                                1\n",
       "speechiness                                    0.0417\n",
       "tempo                                         167.348\n",
       "time_signature                                      4\n",
       "valence                                         0.903"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# caminho do arquivo\n",
    " \n",
    "file_path = 'spotify_dataset.csv'\n",
    "spotify_df = pd.read_csv(file_path)\n",
    "\n",
    "transposed_df = spotify_df.head(1).transpose()\n",
    "# Visualização das primeiras linhas do dataframe\n",
    "transposed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores Ausentes:\n",
      " Nome da Faixa           0\n",
      "Cantor ou Compositor    0\n",
      "Genero                  0\n",
      "Quadrante               0\n",
      "acousticness            0\n",
      "danceability            0\n",
      "energy                  0\n",
      "instrumentalness        0\n",
      "key                     0\n",
      "liveness                0\n",
      "loudness                0\n",
      "mode                    0\n",
      "speechiness             0\n",
      "tempo                   0\n",
      "time_signature          0\n",
      "valence                 0\n",
      "dtype: int64\n",
      "\n",
      "Tipos de Dados:\n",
      " Nome da Faixa            object\n",
      "Cantor ou Compositor     object\n",
      "Genero                   object\n",
      "Quadrante                object\n",
      "acousticness            float64\n",
      "danceability            float64\n",
      "energy                  float64\n",
      "instrumentalness        float64\n",
      "key                       int64\n",
      "liveness                float64\n",
      "loudness                float64\n",
      "mode                      int64\n",
      "speechiness             float64\n",
      "tempo                   float64\n",
      "time_signature            int64\n",
      "valence                 float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Verificando valores ausentes\n",
    "\n",
    "missing_values = spotify_df.isnull().sum()\n",
    "print(\"Valores Ausentes:\\n\", missing_values)\n",
    "\n",
    "# Verificando os tipos de dados\n",
    "\n",
    "print(\"\\nTipos de Dados:\\n\", spotify_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Selecionando as colunas numéricas para normalização\n",
    "num_cols = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'speechiness', 'tempo', 'time_signature', 'valence']\n",
    "\n",
    "# Aplicando a normalização\n",
    "scaler = StandardScaler()\n",
    "spotify_df[num_cols] = scaler.fit_transform(spotify_df[num_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = spotify_df[num_cols]  # Características de áudio \n",
    "y = spotify_df['Quadrante']  # Variável Target\n",
    "\n",
    "# Dividindo os dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Podemos observar se há alguma oportunidade aqui.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanceamento do conjunto de treino, teste e validação para que contenham a mesma proporção\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = spotify_df[num_cols]  # Características de áudio\n",
    "y = spotify_df['Quadrante']  # Variável Target\n",
    "\n",
    "# Dividindo os dados com estratificação\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# divisão em treino e teste temporários\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# divisão do conjunto de treino em treino e validação\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n",
    "\n",
    "\n",
    "# Contagem da variável alvo 'Quadrante' nos conjuntos de treino, teste e validação\n",
    "\n",
    "print(\"Distribuição de classes no conjunto de treino:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"\\nDistribuição de classes no conjunto de teste:\\n\", y_test.value_counts(normalize=True))\n",
    "print(\"\\nDistribuição de classes no conjunto de validação:\\n\", y_val.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Listas para armazenar os resultados\n",
    "neighbors_settings = range(1, 11)  # Testar de 1 a 10 vizinhos\n",
    "accuracy_scores = []\n",
    "\n",
    "# Loop para treinar e testar o KNN com diferentes números de vizinhos\n",
    "for n_neighbors in neighbors_settings:\n",
    "    \n",
    "    # Criar e treinar o modelo\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Fazer previsões no conjunto de teste\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    # Calcular e armazenar a acurácia\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Imprimir o relatório de classificação para cada modelo\n",
    "    print(f'Relatório de classificação para k={n_neighbors}:')\n",
    "    print(classification_report(y_test, y_pred), '\\n')\n",
    "\n",
    "# Plotar os resultados\n",
    "plt.plot(neighbors_settings, accuracy_scores, label='Acurácia do Teste')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.xlabel('Número de Vizinhos')\n",
    "plt.title('KNN: Acurácia vs. Número de Vizinhos')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Identificar o melhor número de vizinhos\n",
    "best_k = neighbors_settings[accuracy_scores.index(max(accuracy_scores))]\n",
    "\n",
    "# Treinar o modelo novamente com o melhor k\n",
    "best_knn_model = KNeighborsClassifier(n_neighbors=best_k)\n",
    "best_knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Salvar o modelo\n",
    "joblib.dump(best_knn_model, r'best_knn_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variando L1 e L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Listas para armazenar os resultados\n",
    "neighbors_settings = range(1, 11)  # Testar de 1 a 10 vizinhos\n",
    "accuracy_scores_l1 = []\n",
    "accuracy_scores_l2 = []\n",
    "\n",
    "# Loop para treinar e testar o KNN com diferentes números de vizinhos e distâncias L1 e L2\n",
    "for n_neighbors in neighbors_settings:\n",
    "    \n",
    "    # KNN com distância L1 (Manhattan)\n",
    "    knn_l1 = KNeighborsClassifier(n_neighbors=n_neighbors, p=1)\n",
    "    knn_l1.fit(X_train, y_train)\n",
    "    y_pred_l1 = knn_l1.predict(X_test)\n",
    "    accuracy_scores_l1.append(accuracy_score(y_test, y_pred_l1))\n",
    "    \n",
    "    # KNN com distância L2 (Euclidiana)\n",
    "    knn_l2 = KNeighborsClassifier(n_neighbors=n_neighbors, p=2)\n",
    "    knn_l2.fit(X_train, y_train)\n",
    "    y_pred_l2 = knn_l2.predict(X_test)\n",
    "    accuracy_scores_l2.append(accuracy_score(y_test, y_pred_l2))\n",
    "\n",
    "# Plotar os resultados\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(neighbors_settings, accuracy_scores_l1, label='L1 - Manhattan', marker='o')\n",
    "plt.plot(neighbors_settings, accuracy_scores_l2, label='L2 - Euclidiana', marker='o')\n",
    "plt.title('KNN: Acurácia vs. Número de Vizinhos com L1 e L2')\n",
    "plt.xlabel('Número de Vizinhos')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considerando que o melhor modelo é um Manhattan com 10 vizinhos, vamos exportá-lo\n",
    "\n",
    "import joblib\n",
    "\n",
    "melhor_knn_model = KNeighborsClassifier(n_neighbors=10, p=1)\n",
    "melhor_knn_model.fit(X_train, y_train)\n",
    "\n",
    "# avaliar o modelo\n",
    "\n",
    "\n",
    "\n",
    "# Salvar o modelo\n",
    "\n",
    "joblib.dump(melhor_knn_model, 'melhor_knn_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAIVE BAYES - V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Hiperparâmetros para testar\n",
    "alphas = np.logspace(-2, 2, 10)  # Para MultinomialNB e BernoulliNB\n",
    "var_smoothing_values = np.logspace(-9, -6, 10)  # Para GaussianNB\n",
    "\n",
    "# Listas para armazenar os resultados\n",
    "accuracy_scores_gaussian = []\n",
    "accuracy_scores_bernoulli = []\n",
    "\n",
    "# Testar GaussianNB\n",
    "for var_smoothing in var_smoothing_values:\n",
    "    model = GaussianNB(var_smoothing=var_smoothing)\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy_scores_gaussian.append(accuracy_score(y_test, model.predict(X_test)))\n",
    "\n",
    "# Testar BernoulliNB\n",
    "for alpha in alphas:\n",
    "    model = BernoulliNB(alpha=alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy_scores_bernoulli.append(accuracy_score(y_test, model.predict(X_test)))\n",
    "\n",
    "# Plotar os resultados para GaussianNB\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(var_smoothing_values, accuracy_scores_gaussian, marker='o')\n",
    "plt.title('GaussianNB: Acurácia vs Var Smoothing')\n",
    "plt.xlabel('Var Smoothing')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.xscale('log')\n",
    "\n",
    "# Plotar os resultados para BernoulliNB\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(alphas, accuracy_scores_bernoulli, marker='o')\n",
    "plt.title('BernoulliNB: Acurácia vs Alpha')\n",
    "plt.xlabel('Alpha')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Inicializando o GaussianNB com o var_smoothing escolhido considerando o ultimo gráfico\n",
    "var_smoothing_optimal = 1e-9  # Exemplo de valor, ajuste conforme necessário\n",
    "best_gaussian_model = GaussianNB(var_smoothing=var_smoothing_optimal)\n",
    "\n",
    "# Treino do modelo com os dados de treinamento\n",
    "best_gaussian_model.fit(X_train, y_train)\n",
    "\n",
    "# previsões no conjunto de teste\n",
    "y_pred = best_gaussian_model.predict(X_test)\n",
    "\n",
    "# Calculo da acurácia\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Acurácia do GaussianNB com var_smoothing de 1e-9: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "# Salve o modelo treinado em um arquivo\n",
    "joblib.dump(best_gaussian_model, 'best_gaussian_model.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### RANDOM FOREST - V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Definir o espaço de hiperparâmetros para testar 10 combinações\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 150, 200, 250, 300, 350, 400, 450],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'max_depth': [10, 20, 30, 40, 50, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Criar o objeto GridSearchCV\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Treinar o modelo com os dados de treinamento e diferentes hiperparâmetros\n",
    "# Este processo pode levar algum tempo dependendo do tamanho do dataset e do número de combinações\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Resultados em um DataFrame para facilitar a visualização e análise\n",
    "results_df = pd.DataFrame(rf_grid_search.cv_results_)\n",
    "\n",
    "# Melhores parâmetros e melhor score\n",
    "best_params = rf_grid_search.best_params_\n",
    "best_score = rf_grid_search.best_score_\n",
    "\n",
    "# Salvar o modelo (não será executado aqui devido à limitação do ambiente)\n",
    "# joblib.dump(rf_grid_search.best_estimator_, 'best_random_forest_model.joblib')\n",
    "\n",
    "# Extraindo apenas os resultados para os hiperparâmetros mais relevantes\n",
    "# e criando gráficos baseados nesses resultados\n",
    "\n",
    "# Filtrando os resultados para n_estimators e max_depth, que são dois dos hiperparâmetros mais impactantes\n",
    "estimators_results = results_df.loc[results_df['param_max_features'] == 'auto']\n",
    "depth_results = results_df.loc[results_df['param_max_features'] == 'auto']\n",
    "\n",
    "# Plotando acurácia vs n_estimators\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.lineplot(data=estimators_results, x='param_n_estimators', y='mean_test_score')\n",
    "plt.title('Random Forest: Acurácia vs Número de Estimadores')\n",
    "plt.xlabel('Número de Estimadores')\n",
    "plt.ylabel('Acurácia Média de Teste')\n",
    "\n",
    "# Plotando acurácia vs max_depth\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.lineplot(data=depth_results, x='param_max_depth', y='mean_test_score')\n",
    "plt.title('Random Forest: Acurácia vs Profundidade Máxima')\n",
    "plt.xlabel('Profundidade Máxima')\n",
    "plt.ylabel('Acurácia Média de Teste')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib\n",
    "\n",
    "# Configurar o modelo KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Treinar o modelo\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "print(\"Matriz de Confusão:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nRelatório de Classificação:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Acurácia: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\n",
    "\n",
    "# Salvar o modelo\n",
    "\n",
    "joblib.dump(knn, 'KNN_MODEL.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Intervalo de valores para n_neighbors\n",
    "neighbors = range(1, 26)\n",
    "\n",
    "# Lista para armazenar as acurácias\n",
    "accuracies = []\n",
    "\n",
    "# Testar o modelo com diferentes valores de n_neighbors\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(neighbors, accuracies, color='blue', marker='o', linestyle='dashed', linewidth=2, markersize=10)\n",
    "plt.title('Acurácia vs. Número de Vizinhos')\n",
    "plt.xlabel('Número de Vizinhos')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Criar uma instância do Gaussian Naive Bayes\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Treinar o modelo\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "print(\"Matriz de Confusão:\\n\", confusion_matrix(y_test, y_pred_nb))\n",
    "print(\"\\nRelatório de Classificação:\\n\", classification_report(y_test, y_pred_nb))\n",
    "print(\"Acurácia: {:.2f}%\".format(accuracy_score(y_test, y_pred_nb) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Obter a matriz de confusão\n",
    "cm = confusion_matrix(y_test, y_pred_nb)\n",
    "\n",
    "# Plotar a matriz de confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "plt.title('Matriz de Confusão do Naive Bayes')\n",
    "plt.ylabel('Classe Verdadeira')\n",
    "plt.xlabel('Classe Predita')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Obter probabilidades preditas\n",
    "probabilidades = nb.predict_proba(X_test)\n",
    "\n",
    "# Para simplificar, vamos plotar as probabilidades de uma classe\n",
    "classe = 0  # ajuste conforme necessário\n",
    "prob_classe = probabilidades[:, classe]\n",
    "\n",
    "# Plotar as probabilidades\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(prob_classe, bins=10, color='skyblue', edgecolor='black')\n",
    "plt.title(f'Probabilidades Preditas para a Classe {classe}')\n",
    "plt.xlabel('Probabilidade')\n",
    "plt.ylabel('Frequência')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter probabilidades preditas\n",
    "probabilidades = nb.predict_proba(X_test)\n",
    "\n",
    "# Número de classes\n",
    "n_classes = probabilidades.shape[1]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Criar um histograma para cada classe\n",
    "for i in range(n_classes):\n",
    "    plt.hist(probabilidades[:, i], bins=20, alpha=0.5, label=f'Classe {i}')\n",
    "\n",
    "plt.title('Probabilidades Preditas para Todas as Classes')\n",
    "plt.xlabel('Probabilidade')\n",
    "plt.ylabel('Frequência')\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Converter as probabilidades em um DataFrame para facilitar a plotagem\n",
    "df_prob = pd.DataFrame(probabilidades, columns=[f'Classe {i}' for i in range(n_classes)])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Criar um gráfico de violino para cada classe\n",
    "sns.violinplot(data=df_prob)\n",
    "plt.title('Distribuição das Probabilidades Preditas para Todas as Classes')\n",
    "plt.ylabel('Probabilidade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Criar uma instância do modelo Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Treinar o modelo\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "print(\"Matriz de Confusão:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"\\nRelatório de Classificação:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"Acurácia: {:.2f}%\".format(accuracy_score(y_test, y_pred_rf) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Obter a matriz de confusão\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Plotar a matriz de confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_rf, annot=True, fmt=\"d\", cmap='Blues')\n",
    "plt.title('Matriz de Confusão do Random Forest')\n",
    "plt.ylabel('Classe Verdadeira')\n",
    "plt.xlabel('Classe Predita')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter a importância das características\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Converter as importâncias em um DataFrame para facilitar a visualização\n",
    "features_df = pd.DataFrame({'Característica': X_train.columns, 'Importância': feature_importances})\n",
    "\n",
    "# Ordenar as características pela importância\n",
    "features_df = features_df.sort_values(by='Importância', ascending=False)\n",
    "\n",
    "# Plotar a importância das características\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importância', y='Característica', data=features_df)\n",
    "plt.title('Importância das Características no Modelo Random Forest')\n",
    "plt.xlabel('Importância')\n",
    "plt.ylabel('Característica')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Definir as combinações de hiperparâmetros para testar\n",
    "param_combinations = [\n",
    "    {'n_estimators': 10, 'max_depth': None, 'min_samples_split': 2},\n",
    "    {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2},\n",
    "    {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 4},\n",
    "    {'n_estimators': 150, 'max_depth': 30, 'min_samples_split': 6},\n",
    "    {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 8}\n",
    "]\n",
    "\n",
    "# Lista para armazenar os resultados\n",
    "results = []\n",
    "\n",
    "# Testar cada combinação de hiperparâmetros\n",
    "for params in param_combinations:\n",
    "    rf = RandomForestClassifier(n_estimators=params['n_estimators'],\n",
    "                                max_depth=params['max_depth'],\n",
    "                                min_samples_split=params['min_samples_split'],\n",
    "                                random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results.append({'params': params, 'accuracy': accuracy})\n",
    "\n",
    "# Converter resultados em um DataFrame\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar os resultados\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='accuracy', y=results_df['params'].astype(str), data=results_df)\n",
    "plt.title('Acurácia do Random Forest para Diferentes Combinações de Hiperparâmetros')\n",
    "plt.xlabel('Acurácia')\n",
    "plt.ylabel('Combinações de Hiperparâmetros')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Criar uma instância do modelo Gradient Boosting\n",
    "gbc = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Treinar o modelo\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_gbc = gbc.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "print(\"Matriz de Confusão:\\n\", confusion_matrix(y_test, y_pred_gbc))\n",
    "print(\"\\nRelatório de Classificação:\\n\", classification_report(y_test, y_pred_gbc))\n",
    "print(\"Acurácia: {:.2f}%\".format(accuracy_score(y_test, y_pred_gbc) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Definir as combinações de hiperparâmetros para testar\n",
    "param_combinations = [\n",
    "    {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 3},\n",
    "    {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 3},\n",
    "    {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 4},\n",
    "    {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 5},\n",
    "    {'n_estimators': 250, 'learning_rate': 0.2, 'max_depth': 4}\n",
    "]\n",
    "\n",
    "# Lista para armazenar os resultados\n",
    "results = []\n",
    "\n",
    "# Testar cada combinação de hiperparâmetros\n",
    "for params in param_combinations:\n",
    "    gbc = GradientBoostingClassifier(n_estimators=params['n_estimators'],\n",
    "                                     learning_rate=params['learning_rate'],\n",
    "                                     max_depth=params['max_depth'],\n",
    "                                     random_state=42)\n",
    "    gbc.fit(X_train, y_train)\n",
    "    y_pred = gbc.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results.append({'params': params, 'accuracy': accuracy})\n",
    "\n",
    "# Converter resultados em um DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar os resultados\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='accuracy', y=results_df['params'].astype(str), data=results_df)\n",
    "plt.title('Acurácia do Gradient Boosting para Diferentes Combinações de Hiperparâmetros')\n",
    "plt.xlabel('Acurácia')\n",
    "plt.ylabel('Combinações de Hiperparâmetros')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Inicializar o LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Ajustar e transformar as classes\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Criar uma instância do modelo XGBoost\n",
    "xgb_clf = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Treinar o modelo XGBoost com as classes codificadas\n",
    "xgb_clf.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo (lembre-se de decodificar as previsões de volta para as classes originais se necessário)\n",
    "print(\"Matriz de Confusão:\\n\", confusion_matrix(y_test_encoded, y_pred_xgb))\n",
    "print(\"\\nRelatório de Classificação:\\n\", classification_report(y_test_encoded, y_pred_xgb))\n",
    "print(\"Acurácia: {:.2f}%\".format(accuracy_score(y_test_encoded, y_pred_xgb) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Definir as combinações de hiperparâmetros para testar\n",
    "param_combinations = [\n",
    "    {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 3},\n",
    "    {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 4},\n",
    "    {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 5},\n",
    "    {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 6},\n",
    "    {'n_estimators': 250, 'learning_rate': 0.2, 'max_depth': 7}\n",
    "]\n",
    "\n",
    "# Lista para armazenar os resultados\n",
    "results = []\n",
    "\n",
    "# Testar cada combinação de hiperparâmetros\n",
    "for params in param_combinations:\n",
    "    xgb_model = XGBClassifier(n_estimators=params['n_estimators'],\n",
    "                              learning_rate=params['learning_rate'],\n",
    "                              max_depth=params['max_depth'],\n",
    "                              random_state=42)\n",
    "    xgb_model.fit(X_train, y_train_encoded)\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "    results.append({'params': params, 'accuracy': accuracy})\n",
    "\n",
    "# Converter resultados em um DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar os resultados\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='accuracy', y=results_df['params'].astype(str), data=results_df)\n",
    "plt.title('Acurácia do XGBoost para Diferentes Combinações de Hiperparâmetros')\n",
    "plt.xlabel('Acurácia')\n",
    "plt.ylabel('Combinações de Hiperparâmetros')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Neurais ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Inicializar o LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Ajustar e transformar as classes\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Criar o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))  # Usando 4 porque temos 4 classes (Q1, Q2, Q3, Q4)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Avaliar o modelo\n",
    "loss, accuracy = model.evaluate(X_test, y_test_encoded, verbose=0)\n",
    "print(\"Acurácia: {:.2f}%\".format(accuracy * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotar a acurácia do treinamento e validação\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Acurácia (treinamento)')\n",
    "plt.plot(history.history['val_accuracy'], label='Acurácia (validação)')\n",
    "plt.title('Acurácia ao Longo das Épocas')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.xlabel('Época')\n",
    "plt.legend()\n",
    "\n",
    "# Plotar a perda do treinamento e validação\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Perda (treinamento)')\n",
    "plt.plot(history.history['val_loss'], label='Perda (validação)')\n",
    "plt.title('Perda ao Longo das Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.xlabel('Época')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNA - Mais profunda ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "# Criar o modelo\n",
    "model_complex = Sequential()\n",
    "\n",
    "# Primeira camada oculta\n",
    "model_complex.add(Dense(128, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model_complex.add(BatchNormalization())\n",
    "model_complex.add(Dropout(0.5))\n",
    "\n",
    "# Segunda camada oculta\n",
    "model_complex.add(Dense(64, activation='relu'))\n",
    "model_complex.add(BatchNormalization())\n",
    "model_complex.add(Dropout(0.5))\n",
    "\n",
    "# Terceira camada oculta\n",
    "model_complex.add(Dense(32, activation='relu'))\n",
    "model_complex.add(BatchNormalization())\n",
    "model_complex.add(Dropout(0.5))\n",
    "\n",
    "# Camada de saída\n",
    "model_complex.add(Dense(4, activation='softmax'))  # 4 classes de saída\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_complex.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "history_complex = model_complex.fit(X_train, y_train_encoded, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Avaliar o modelo\n",
    "loss, accuracy = model_complex.evaluate(X_test, y_test_encoded, verbose=0)\n",
    "print(\"Acurácia: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotar a acurácia do treinamento e validação\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_complex.history['accuracy'], label='Acurácia (treinamento)')\n",
    "plt.plot(history_complex.history['val_accuracy'], label='Acurácia (validação)')\n",
    "plt.title('Acurácia ao Longo das Épocas')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.xlabel('Época')\n",
    "plt.legend()\n",
    "\n",
    "# Plotar a perda do treinamento e validação\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_complex.history['loss'], label='Perda (treinamento)')\n",
    "plt.plot(history_complex.history['val_loss'], label='Perda (validação)')\n",
    "plt.title('Perda ao Longo das Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.xlabel('Época')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# É comum usar pipelines para combinar a etapa de pré-processamento com o modelo\n",
    "svm_pipeline = make_pipeline(StandardScaler(), SVC(probability=True))\n",
    "\n",
    "# Treinar o modelo com os dados de treinamento\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Previsões no conjunto de Teste\n",
    "\n",
    "y_pred_svm = svm_pipeline.predict(X_test)\n",
    "\n",
    "# Avaliação do modelo\n",
    "\n",
    "print(\"Matriz de Confusão:\\n\", confusion_matrix(y_test, y_pred_svm))\n",
    "print(\"\\nRelatório de Classificação:\\n\", classification_report(y_test, y_pred_svm))\n",
    "print(\"Acurácia: {:.2f}%\".format(accuracy_score(y_test, y_pred_svm) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Definir o espaço de hiperparâmetros para testar\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],             # Regularização\n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],   # Parâmetro do Kernel\n",
    "    'kernel': ['rbf']                          # Tipo do Kernel\n",
    "}\n",
    "\n",
    "# Criar o objeto GridSearchCV\n",
    "grid_search = GridSearchCV(SVC(), param_grid, refit=True, verbose=3, cv=5)\n",
    "\n",
    "# Treinar o modelo SVM com os dados de treinamento e diferentes hiperparâmetros\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Melhores parâmetros e melhor score\n",
    "print(\"Melhores Parâmetros:\", grid_search.best_params_)\n",
    "print(\"Melhor Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reshape the mean test scores into a 5x5 grid for the values of 'C' and 'gamma'\n",
    "scores = results['mean_test_score'].reshape(5, 5)\n",
    "\n",
    "# Plot a heatmap\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(scores, annot=True, xticklabels=param_grid['gamma'], yticklabels=param_grid['C'], cmap='viridis')\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('C')\n",
    "plt.title('Acurácia do GridSearchCV para o SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "# Acessar o melhor estimador encontrado pelo GridSearchCV\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "# Salvar o melhor modelo\n",
    "joblib.dump(best_svm_model, 'best_svm_model.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uspspotenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
